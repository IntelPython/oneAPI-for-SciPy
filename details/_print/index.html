<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.101.0"><link rel=canonical type=text/html href=https://IntelPython.github.io/oneAPI-for-SciPy/details/><link rel=alternate type=application/rss+xml href=https://IntelPython.github.io/oneAPI-for-SciPy/details/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/oneAPI-for-SciPy/favicons/favicon.ico><link rel=apple-touch-icon href=/oneAPI-for-SciPy/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-192x192.png sizes=192x192><title>SciPy 2022 virtual poster | oneAPI for SciPy</title><meta name=description content="oneAPI for Scientific Python community
by Diptorup Deb and Oleksandr Pavlyk, Intel Corporation

With this poster we would like to inform Scientific …"><meta property="og:title" content="SciPy 2022 virtual poster"><meta property="og:description" content="The most popular HTML, CSS, and JS library in the world."><meta property="og:type" content="website"><meta property="og:url" content="https://IntelPython.github.io/oneAPI-for-SciPy/details/"><meta itemprop=name content="SciPy 2022 virtual poster"><meta itemprop=description content="The most popular HTML, CSS, and JS library in the world."><meta name=twitter:card content="summary"><meta name=twitter:title content="SciPy 2022 virtual poster"><meta name=twitter:description content="The most popular HTML, CSS, and JS library in the world."><link rel=preload href=/oneAPI-for-SciPy/scss/main.min.04b151beccbf4ac02d79f0b15e74ad3db29ff903370abdffbd69dd7a531dbc0f.css as=style><link href=/oneAPI-for-SciPy/scss/main.min.04b151beccbf4ac02d79f0b15e74ad3db29ff903370abdffbd69dd7a531dbc0f.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-00000000-0","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/oneAPI-for-SciPy/><span class=navbar-logo></span><span class=font-weight-bold>oneAPI for SciPy</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/oneAPI-for-SciPy/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/oneAPI-for-SciPy/details/><span class=active>Details</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/oneAPI-for-SciPy/details/>Return to the regular view of this page</a>.</p></div><h1 class=title>SciPy 2022 virtual poster</h1><ul><li>1: <a href=#pg-cba4ed306c91f9bf161ff01ffac19463>Quickstart</a></li><li>2: <a href=#pg-430d39d3eb6a31517cc3fc250eafde0d>What is oneAPI</a></li><li>3: <a href=#pg-4498e1017b824e5369db44363539378c>Install Intel(R) oneAPI toolkits</a></li><li>4: <a href=#pg-cd3f9e926b7bc350bedceab59f4bbef9>oneAPI Python extensions</a></li><li>5: <a href=#pg-0eac71da689dcff3ca0b49e2618ca8f8>Why use oneAPI in Python</a></li></ul><div class=content><h2 id=oneapi-for-scientific-python-community>oneAPI for Scientific Python community</h2><p>by <em>Diptorup Deb</em> and <em>Oleksandr Pavlyk</em>, Intel Corporation</p><hr><p>With this poster we would like to inform Scientific Python community about oneAPI programming model for heterogeneous systems
and how to leverage it for the benefit of Python users.</p><p>We hope to interest Python extension authors to start developing portable accelerator aware Python packages using oneAPI.
This poster presents the tooling to build Python extensions with DPC++, as well as Python binding to DPC++ runtime classes
implemented in <code>dpctl</code>.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-cba4ed306c91f9bf161ff01ffac19463>1 - Quickstart</h1><div class=lead>Get started with oneAPI Python-extensions</div><p>In a heterogeneous system there may be <strong>multiple</strong> devices a Python user may want to engage.
For example, it is common for a consumer-grade laptop to feature an integrated or a discrete
GPU alongside a CPU.</p><p>To harness their power one needs to know how to answer the following 3 key questions:</p><ol><li>How does a Python program recognize available computational devices?</li><li>How does a Python workload specify computations to be offloaded to selected devices?</li><li>How does a Python application manage data sharing?</li></ol><h3 id=recognizing-available-devices>Recognizing available devices</h3><p>Python package <code>dpctl</code> answers these questions. All the computational devices known to the
underlying DPC++ runtime can be accessed using <code>dpctl.get_devices()</code>. A specific device of
interest <a href=https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/device_selection.html>can be selected</a> either using a helper function,
e.g. <code>dpctl.select_gpu_device()</code>, or by passing a filter selector string
to <code>dpctl.SyclDevice</code> constructor.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>dpctl</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># select a GPU device. If multiple devices present, </span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># let the underlying runtime select from GPUs</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_gpu</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclDevice</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;gpu&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># select a CPU device</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_cpu</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclDevice</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;cpu&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># stand-alone function, equivalent to C++ </span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#   `auto dev = sycl::gpu_selector().select_device();`</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_gpu_alt</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>select_gpu_device</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># stand-alone function, equivalent to C++ </span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#   `auto dev = sycl::cpu_selector().select_device();`</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_cpu_alt</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>select_cpu_device</span><span style=color:#000;font-weight:700>()</span>
</span></span></code></pre></div><p>A <a href=https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/devices.html>device object</a> can be used to query properies of the device, such as its name, vendor, maximal number of computational units, memory size, etc.</p><h3 id=specifying-offload-target>Specifying offload target</h3><p>To answer the second question on the list we need a digression to explain offloading in DPC++ first. A computational task is offloaded for execution on a device
by submitting it to DPC++ runtime which inserts the task in a computational graph. Once the device becomes available the runtime selects a task whose dependencies
are met for execution.</p><p>The computational graph along with the device targeted by its tasks are stored in a <a href=https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/queues.html>SYCL queue</a> object. The task submission is therefore always
associated with a queue.</p><p>Queues can be constructed directly from a device object, or by using a filter selector string to indicate the device to construct:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8f5902;font-style:italic># construct queue from device object</span>
</span></span><span style=display:flex><span><span style=color:#000>q1</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclQueue</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>dev_gpu</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># construct queue using filter selector</span>
</span></span><span style=display:flex><span><span style=color:#000>q2</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclQueue</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;gpu&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>The computational tasks can be stored in an oneAPI native extension in which case their submission is orchestrated
during Python API calls. Let&rsquo;s consider a function that offloads an evaluation of a polynomial for every point of a
NumPy array <code>X</code>. Such a function needs to receive a queue object to indicate which device the computation must be
offloaded too:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8f5902;font-style:italic># allocate space for the result</span>
</span></span><span style=display:flex><span><span style=color:#000>Y</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>empty_like</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># evaluate polynomial on the device targeted by the queue, Y[i] = p(X[i])</span>
</span></span><span style=display:flex><span><span style=color:#000>onapi_ext</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>offloaded_poly_evaluate</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>q</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>Python call to <code>onapi_ext.offloaded_poly_evaluate</code> of NumPy arrays of double precision floating pointer numbers gets
translated to the following sample C++ code:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++><span style=display:flex><span><span style=color:#204a87;font-weight:700>void</span> 
</span></span><span style=display:flex><span><span style=color:#000>cpp_offloaded_poly_evaluate</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>  <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>queue</span> <span style=color:#000>q</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>const</span> <span style=color:#204a87;font-weight:700>double</span> <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>double</span> <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000>Y</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>size_t</span> <span style=color:#000>n</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>    
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic>// create buffers from malloc allocations to make data accessible from device
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>    <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>buffer</span><span style=color:#ce5c00;font-weight:700>&lt;</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>double</span><span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#000>buf_X</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>n</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>    <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>buffer</span><span style=color:#ce5c00;font-weight:700>&lt;</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>double</span><span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#000>buf_Y</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>Y</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>n</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#000>q</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>submit</span><span style=color:#000;font-weight:700>([</span><span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>handler</span> <span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>cgh</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#8f5902;font-style:italic>// create buffer accessors indicating kernel data-flow pattern  
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>        <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>accessor</span> <span style=color:#000>acc_X</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>buf_X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>cgh</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>read_only</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>        <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>accessor</span> <span style=color:#000>acc_Y</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>buf_Y</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>cgh</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>write_only</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>no_init</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#000>cgh</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>parallel_for</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>n</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>           <span style=color:#8f5902;font-style:italic>// lambda function that gets executed by different work-items with 
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>           <span style=color:#8f5902;font-style:italic>// different arguments in parallel
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>           <span style=color:#000;font-weight:700>[</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>id</span><span style=color:#ce5c00;font-weight:700>&lt;</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#000>id</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>              <span style=color:#204a87;font-weight:700>auto</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>accX</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>id</span><span style=color:#000;font-weight:700>];</span>
</span></span><span style=display:flex><span>              <span style=color:#000>accY</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>id</span><span style=color:#000;font-weight:700>]</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>3.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>1.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>-</span><span style=color:#0000cf;font-weight:700>0.5</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#0000cf;font-weight:700>0.3</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>x</span><span style=color:#000;font-weight:700>));</span>
</span></span><span style=display:flex><span>           <span style=color:#000;font-weight:700>});</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}).</span><span style=color:#000>wait</span><span style=color:#000;font-weight:700>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>return</span><span style=color:#000;font-weight:700>;</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><p>We refer the reader to excellent freely available &ldquo;<a href=https://link.springer.com/book/10.1007%2F978-1-4842-5574-2>Data Parallel C++</a>&rdquo; book for details of this C++ snippet.</p><p>Our package <code>numba_dpex</code> allows one to write kernels in Python as well.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>numba_dpex</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5c35cc;font-weight:700>@numba_dpex</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>kernel</span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>numba_dpex_poly</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>    <span style=color:#000>i</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>numba_dpex</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get_global_id</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>X</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>i</span><span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span>    <span style=color:#000>Y</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>i</span><span style=color:#000;font-weight:700>]</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>3.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>1.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>-</span><span style=color:#0000cf;font-weight:700>0.5</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#0000cf;font-weight:700>0.3</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>x</span><span style=color:#000;font-weight:700>))</span>
</span></span></code></pre></div><p>Specifying the execution queue is done using Python context manager:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>numpy</span> <span style=color:#204a87;font-weight:700>as</span> <span style=color:#000>np</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>X</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>random</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>randn</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>10</span><span style=color:#ce5c00;font-weight:700>**</span><span style=color:#0000cf;font-weight:700>6</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#000>Y</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>empty_like</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>with</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>device_context</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>q</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic># apply the kernel to elements of X, writing value into Y, </span>
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic># while executing using given queue</span>
</span></span><span style=display:flex><span>    <span style=color:#000>numba_dpex_poly</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>X</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>numba_dpex</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>DEFAULT_LOCAL_SIZE</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>The argument to <code>device_context</code> can be a queue object, a device object for which a temporary queue will be created,
or a filter selector string. Thus we could have equally used <code>dpctl.device_context(gpu_dev)</code> or <code>dpctl.device_context("gpu")</code>.</p><p>Note that in this examples data sharing was implicitly managed for us: in the case of calling a function from a precompiled
oneAPI native extension data sharing was managed by DPC++ runtime, while in the case of using <code>numba_dpex</code> kernel it was managed
during execution of <code>__call__</code> method.</p><h3 id=data-sharing>Data sharing</h3><p>Implicit data managing is surely convenient, but its use in interpreted code comes at a performance cost. A runtime must
implicitly copy data from host to the device before the kernel execution commences and then copy some (or all) of it back
after the execution completes.</p><p><code>dpctl</code> provides for allocating memory directly accessible to kernels executing on a device using SYCL&rsquo;s
Unified Shared Memory (<a href=https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:usm>USM</a>) feature as well as for the <a href=https://data-apis.org/array-api/latest/>array-API</a> conforming ND-array
object <code>dpctl.tensor.usm_ndarray</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>dpctl.tensor</span> <span style=color:#204a87;font-weight:700>as</span> <span style=color:#000>dpt</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># allocate array of doubles using USM-device allocation on GPU device</span>
</span></span><span style=display:flex><span><span style=color:#000>X</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpt</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>arange</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>0.</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>end</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>step</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1e-6</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>device</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;gpu&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>usm_type</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;device&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># allocate array for the output</span>
</span></span><span style=display:flex><span><span style=color:#000>Y</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpt</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>empty_like</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># execution queue is inferred from allocation queues.</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Kernel is executed on the same device where arrays were allocated</span>
</span></span><span style=display:flex><span><span style=color:#000>numba_dpex_poly</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>X</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>numba_dpex</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>DEFAULT_LOCAL_SIZE</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>The execution queue can be unambiguously determined in this case because both arguments are
USM arrays with the same allocation queues because <code>X.sycl_queue == Y.sycl_queue</code> evaluates to <code>True</code>.
Should the queues be different, such an inference becomes impossible and <code>numba_dpex</code> raises
<code>IndeterminateExecutionQueueError</code> advising user to explicitly migrate the data.</p><p>Migration can be accomplished either by using <code>dpctl.tensor.asarray(X, device=target_device)</code>
to create a copy, or by using <code>X.to_device(target_device)</code> method.</p><p>The result USM array can be copied back into a NumPy array using <code>dpt.asnumpy(Y)</code> if needed.</p><p><code>dpctl</code> and <code>numba_dpex</code> are both under heavy development. Feel free to file an issue on GitHub or
reach out on Gitter should you encounter any issues.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-430d39d3eb6a31517cc3fc250eafde0d>2 - What is oneAPI</h1><div class=lead>oneAPI - the standard and its implementation.</div><p><a href=https://www.oneapi.io>oneAPI</a> is an open standard for a unified application
programming interface (API) that delivers a common developer experience across
accelerator architectures, including multi-core CPUs, GPUs, and FPGAs.</p><p>A freely available implementation of the standard is available through
<a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/toolkits.html>Intel(R) oneAPI Toolkits</a>. The <a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html>Intel(R) Base Toolkit</a> features
an industry-leading C++ compiler that implements <a href=https://www.khronos.org/sycl/>SYCL*</a>, an evolution of C++
for heterogeneous computing. It also includes a suite of performance libraries, such as
Intel(R) oneAPI Math Kernel Library (<a href=https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/api-based-programming/intel-oneapi-math-kernel-library-onemkl.html>oneMKL</a>), etc, as well as
<a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-for-python.html>Intel(R) Distribution for Python*</a>.</p><p><img src=../../images/oneapi_basekit.webp alt="Intel Base Toolkit" title="Composition of Intel Base Toolkit"></p><p>DPC++ is a LLVM-based compiler project that implements compiler and runtime support for SYCL* language.
It is being developed in <code>sycl</code> branch of the LLVM project fork <a href=https://github.com/intel/llvm.git>github.com/intel/llvm</a>.
The project publishes <a href=https://github.com/intel/llvm/releases>daily builds</a> for Linux.</p><p>Intel(R) oneAPI DPC++ compiler is a proprietary product that builds on the open-source DPC++ project.
It is part of Intel(R) compiler suite which has completed the <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/adoption-of-llvm-complete-icx.html>adoption of LLVM infrastructure</a> and is available in oneAPI toolkits.
In particular, Intel(R) Fortran compiler is freely avialable on all supported platforms in <a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit.html>Intel(R) oneAPI HPC Toolkit</a>.</p><p>DPC++ leverages standard toolchain runtime libraries, such as <code>glibc</code> and <code>libstdc++</code> on Linux and <code>wincrt</code> on Windows. This makes it possible to use
Intel C/C++ compilers, including DPC++, to compile Python <a href=skbuild.mc>native extensions</a> compatible with the CPython and the rest of Python stack.</p><p>In order to enable cross-architecture programming for CPUs and accelerators the DPC++ runtime adopted <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/expanding-oneapi-support-for-languages-and-accelerators.html>layered architecture</a>.
Software concepts are mapped to hardware abstraction layer by user-specified <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/five-outstanding-additions-sycl2020.html>SYCL backend</a> which programs the specific hardware in use.</p><p>An integral part of this layered architecture is provided by <a href=https://github.com/intel/compute-runtime.git>Intel(R) Compute Runtime</a>. oneAPI application is a fat binary consisting of
device codes in a standardized intermediate form <a href=https://www.khronos.org/spir/>SPIR-V</a> and host code which orchestrates tasks such as querying of the heterogeneous system it is
running on, selecting accelerator(s), compiling (jitting) device code in the intermediate representation for the selected device, managing device memory, and
submitting compiled device code for execution. The host code performs these tasks by using DPC++ runtime, which maps them to hardware abstraction layer, that
talks to hardware-specific drivers.</p><p><img src=../../images/oneAPI-executable-diagram.webp alt="working of oneAPI executable"></p><h2 id=additional-information>Additional information</h2><p><a href=https://link.springer.com/book/10.1007%2F978-1-4842-5574-2>Data Parallel C++ book</a> is an excellent resource to get familiar with programming
heterogeneous systems using C++ and SYCL*.</p><p>Intel(R) DevCloud hosts <a href=https://devcloud.intel.com/oneapi/get_started/baseTrainingModules/>base training material</a> which can be executed
on the variety of Intel(R) hardware using preinstalled oneAPI toolkits.</p><p>Julia has support for oneAPI <a href=https://github.com/JuliaGPU/oneAPI.jl>github.com/JuliaGPU/oneAPI.jl</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4498e1017b824e5369db44363539378c>3 - Install Intel(R) oneAPI toolkits</h1><div class=lead>Pointers about how to get Intel(R) oneAPI toolkits.</div><h1 id=installation-of-intelr-oneapi-toolkits>Installation of Intel(R) oneAPI toolkits</h1><h2 id=use-intelr-devcloud>Use Intel(R) DevCloud</h2><p><a href=https://devcloud.intel.com/oneapi/>Get free access</a> to a development sandbox with preinstalled and configured
oneAPI toolkits as well as access variety of Intel hardware. This is great and low effort way
to start exploring oneAPI. We recommend to start with <a href="https://jupyter.oneapi.devcloud.intel.com/hub/login?next=/lab/tree/Welcome.ipynb?reset">Jupyter Lab</a>.</p><h2 id=install-locally>Install locally</h2><p>To add oneAPI to your local toolbox, download and install the basekit for your operating system from <a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html>download page</a>.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>For Linux*, toolkits can be installed using OS&rsquo;s package managers, as well as tried out from within a docker-container. Please refer
to the download page for specifics.</div><p>Make sure to configure your system by following steps from &ldquo;<a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html#gs.3mwueb>Get Started Guide</a>&rdquo;
document applicable for your operating system.</p><h2 id=install-in-ci>Install in CI</h2><p>oneAPI can be installed into Linux-powered CI by using the OS&rsquo;s package manager and installing
only the necessary components from required toolkits.</p><p>See <a href=https://github.com/IntelPython/dpctl/blob/1f8e4b35c3d623bd7e0d84dad32f421aef34ac0f/.github/workflows/generate-docs.yml#L18-L29>this example</a> of installing DPC++ compiler in GitHub actions
for <a href=https://github.com/IntelPython/dpctl>IntelPython/dpctl</a> project.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cd3f9e926b7bc350bedceab59f4bbef9>4 - oneAPI Python extensions</h1><div class=lead>Python extensions can be built with DPC++. This is how.</div><h1 id=oneapi-python-extensions>oneAPI Python extensions</h1><p>DPC++ is a single source compiler. It generates <a href=https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/programming-interface/compilation-flow-overview.html>both the host code and the device code</a> in a single fat binary.
DPC++ is an LLVM-based compiler, but the host portion of the binary it produces is compatible with GCC runtime libraries on Linux and Windows runtime libraries on Windows. Thus native Python extensions authored in C++ can be directly built with DPC++.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0eac71da689dcff3ca0b49e2618ca8f8>5 - Why use oneAPI in Python</h1><p>Here is a summary of why we think Scientific Python community should embrace oneAPI</p><ol><li>oneAPI is an open, cross-industry, standards-based, unified, multiarchitecture, multi-vendor <a href=https://en.wikipedia.org/wiki/OneAPI_(compute_acceleration)>programming model</a>.</li><li>DPC++ compiler is being developed in open-source, see <a href=http://github.com/intel/llvm>http://github.com/intel/llvm</a>, and is being upstreamed into LLVM project itself.</li><li>Open source compiler supports <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/five-outstanding-additions-sycl2020.html>variety of backends</a>, including oneAPI <a href=https://spec.oneapi.io/level-zero/latest/index.html>Level-Zero</a>, <a href=https://www.khronos.org/opencl/>OpenCL(TM)</a>, NVIDIA(R) <a href=https://developer.nvidia.com/cuda-toolkit>CUDA(R)</a>, and <a href=https://github.com/ROCm-Developer-Tools/HIP>HIP</a>.</li><li><a href=https://github.com/oneapi-src/oneMKL>oneAPI Math Kernel Library (oneMKL) Interfaces</a> supports a collection of third-party libraries associated with
supported backends permitting portability.</li></ol><p>With these features in mind, and DPC++ runtime being compatible with compiler toolchain used to build CPython itself, use of oneAPI
promises to enable Python extensions to leverage a variety of accelerators, while maintaining portability
of Python extensions across different heterogenous systems, from HPC clusters and servers to laptops.</p></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Gitter aria-label=Gitter><a class=text-white target=_blank rel=noopener href=https://gitter.im/Data-Parallel-Python/community aria-label=Gitter><i class="fab fa-gitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow for Intel oneAPI" aria-label="Stack Overflow for Intel oneAPI"><a class=text-white target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/intel-oneapi aria-label="Stack Overflow for Intel oneAPI"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/IntelPython/ aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Intel Developer Zone" aria-label="Intel Developer Zone"><a class=text-white target=_blank rel=noopener href=https://community.intel.com/t5/Intel-Distribution-for-Python/bd-p/distribution-python aria-label="Intel Developer Zone"><i class="fas fa-hands-helping"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2022 Intel Corp. All Rights Reserved</small></div></div></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/oneAPI-for-SciPy/js/main.min.57829c4a449c0d377b7a44a808f57cbadb16a81488df2545e1739bf4e356c76d.js integrity="sha256-V4KcSkScDTd7ekSoCPV8utsWqBSI3yVF4XOb9ONWx20=" crossorigin=anonymous></script></body></html>