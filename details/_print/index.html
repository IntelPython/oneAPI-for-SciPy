<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.111.3"><link rel=canonical type=text/html href=https://IntelPython.github.io/oneAPI-for-SciPy/details/><link rel=alternate type=application/rss+xml href=https://IntelPython.github.io/oneAPI-for-SciPy/details/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/oneAPI-for-SciPy/favicons/favicon.ico><link rel=apple-touch-icon href=/oneAPI-for-SciPy/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/oneAPI-for-SciPy/favicons/android-192x192.png sizes=192x192><title>SciPy 2022 virtual poster | oneAPI for SciPy</title><meta name=description content="oneAPI for Scientific Python community
by Diptorup Deb and Oleksandr Pavlyk, Intel Corporation

With this poster we would like to inform Scientific …"><meta property="og:title" content="SciPy 2022 virtual poster"><meta property="og:description" content="The most popular HTML, CSS, and JS library in the world."><meta property="og:type" content="website"><meta property="og:url" content="https://IntelPython.github.io/oneAPI-for-SciPy/details/"><meta itemprop=name content="SciPy 2022 virtual poster"><meta itemprop=description content="The most popular HTML, CSS, and JS library in the world."><meta name=twitter:card content="summary"><meta name=twitter:title content="SciPy 2022 virtual poster"><meta name=twitter:description content="The most popular HTML, CSS, and JS library in the world."><link rel=preload href=/oneAPI-for-SciPy/scss/main.min.ffe34f5b5cbe0b99b506df305f2753971f4ec26ec09729e253dbba53e0b8a628.css as=style><link href=/oneAPI-for-SciPy/scss/main.min.ffe34f5b5cbe0b99b506df305f2753971f4ec26ec09729e253dbba53e0b8a628.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-00000000-0","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/oneAPI-for-SciPy/><span class=navbar-logo></span><span class=font-weight-bold>oneAPI for SciPy</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/oneAPI-for-SciPy/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/oneAPI-for-SciPy/details/><span class=active>Details</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/oneAPI-for-SciPy/details/>Return to the regular view of this page</a>.</p></div><h1 class=title>SciPy 2022 virtual poster</h1><ul><li>1: <a href=#pg-58d971cf642f5542cbec54c516dfbf60>Programming model</a></li><li>2: <a href=#pg-430d39d3eb6a31517cc3fc250eafde0d>What is oneAPI</a></li><li>3: <a href=#pg-5623910d18a8223f7729c493aeec874c>Features Summary</a></li><li>4: <a href=#pg-4498e1017b824e5369db44363539378c>Install Intel(R) oneAPI toolkits</a></li><li>5: <a href=#pg-cd3f9e926b7bc350bedceab59f4bbef9>oneAPI Python extensions</a></li><li>6: <a href=#pg-a87383bce703fba3e69097c5c1cb8546>Programming Model</a></li><li>7: <a href=#pg-0eac71da689dcff3ca0b49e2618ca8f8>Why use oneAPI in Python</a></li></ul><div class=content><h2 id=oneapi-for-scientific-python-community>oneAPI for Scientific Python community</h2><p>by <em>Diptorup Deb</em> and <em>Oleksandr Pavlyk</em>, Intel Corporation</p><hr><p>With this poster we would like to inform Scientific Python community about
oneAPI programming model for heterogeneous systems and how to leverage it for
the benefit of Python users.</p><p>We hope to interest Python extension authors to start developing portable
accelerator-aware Python packages using oneAPI. This poster presents the tooling
to build Python extensions with DPC++, as well as Python binding to DPC++
runtime classes implemented in <code>dpctl</code>.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-58d971cf642f5542cbec54c516dfbf60>1 - Programming model</h1><div class=lead>Get started with oneAPI Python-extensions</div><p>In a heterogeneous system there may be <strong>multiple</strong> devices a Python user may want to engage.
For example, it is common for a consumer-grade laptop to feature an integrated or a discrete
GPU alongside a CPU.</p><p>To harness their power one needs to know how to answer the following 3 key questions:</p><ol><li>How does a Python program recognize available computational devices?</li><li>How does a Python workload specify computations to be offloaded to selected devices?</li><li>How does a Python application manage data sharing?</li></ol><h3 id=recognizing-available-devices>Recognizing available devices</h3><p>Python package <code>dpctl</code> answers these questions. All the computational devices known to the
underlying DPC++ runtime can be accessed using <code>dpctl.get_devices()</code>. A specific device of
interest <a href=https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/device_selection.html>can be selected</a> either using a helper function,
e.g. <code>dpctl.select_gpu_device()</code>, or by passing a filter selector string
to <code>dpctl.SyclDevice</code> constructor.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>dpctl</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># select a GPU device. If multiple devices present, </span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># let the underlying runtime select from GPUs</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_gpu</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclDevice</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;gpu&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># select a CPU device</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_cpu</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclDevice</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;cpu&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># stand-alone function, equivalent to C++ </span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#   `auto dev = sycl::gpu_selector().select_device();`</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_gpu_alt</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>select_gpu_device</span><span style=color:#000;font-weight:700>()</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># stand-alone function, equivalent to C++ </span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#   `auto dev = sycl::cpu_selector().select_device();`</span>
</span></span><span style=display:flex><span><span style=color:#000>dev_cpu_alt</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>select_cpu_device</span><span style=color:#000;font-weight:700>()</span>
</span></span></code></pre></div><p>A <a href=https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/devices.html>device object</a> can be used to query properies of the device, such as its name, vendor, maximal number of computational units, memory size, etc.</p><h3 id=specifying-offload-target>Specifying offload target</h3><p>To answer the second question on the list we need a digression to explain offloading in oneAPI DPC++ first.</p><div class="alert alert-secondary" role=alert><h4 class=alert-heading>Offloading in oneAPI DPC++</h4><p>In DPC++, a computation kernel can be specified using generic C++ programming and then the kernel can be offloaded to any device that is supported by an underlying SYCL runtime. The device to which the kernel is offloaded is specified using an <strong>execution queue</strong> when <em>launching the kernel</em>.</p><p>The oneAPI unified programming model brings portability across heterogeneous architectures. Another important aspect of the programming model is its inherent flexibility that makes it possible to go beyond portability and even strive for performance portability. An oneAPI library may be implemented using C++ techniques such as template metaprogramming or dynamic polymorphism to implement specializations for a generic kernel. If a kernel is implemented polymorphically, the specialized implementation will be dispatched based on the execution queue specified during kernel launch. The oneMKL library is an example of a performance portable oneAPI library.</p></div><p>A computational task is offloaded for execution on a device by submitting it to DPC++ runtime which inserts the task in a computational graph.
Once the device becomes available the runtime selects a task whose dependencies are met for execution. The computational graph as well as the device targeted by its tasks are stored in a <a href=https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/queues.html>SYCL queue</a> object. The task submission is therefore always
associated with a queue.</p><p>Queues can be constructed directly from a device object, or by using a filter selector string to indicate the device to construct:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8f5902;font-style:italic># construct queue from device object</span>
</span></span><span style=display:flex><span><span style=color:#000>q1</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclQueue</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>dev_gpu</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># construct queue using filter selector</span>
</span></span><span style=display:flex><span><span style=color:#000>q2</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclQueue</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;gpu&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>The computational tasks can be stored in an oneAPI native extension in which case their submission is orchestrated
during Python API calls. Let&rsquo;s consider a function that offloads an evaluation of a polynomial for every point of a
NumPy array <code>X</code>. Such a function needs to receive a queue object to indicate which device the computation must be
offloaded to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8f5902;font-style:italic># allocate space for the result</span>
</span></span><span style=display:flex><span><span style=color:#000>Y</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>empty_like</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># evaluate polynomial on the device targeted by the queue, Y[i] = p(X[i])</span>
</span></span><span style=display:flex><span><span style=color:#000>onapi_ext</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>offloaded_poly_evaluate</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>exec_q</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>Python call to <code>onapi_ext.offloaded_poly_evaluate</code> applied to NumPy arrays of double precision floating pointer numbers gets
translated to the following sample C++ code:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c++ data-lang=c++><span style=display:flex><span><span style=color:#204a87;font-weight:700>void</span> 
</span></span><span style=display:flex><span><span style=color:#000>cpp_offloaded_poly_evaluate</span><span style=color:#000;font-weight:700>(</span>
</span></span><span style=display:flex><span>  <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>queue</span> <span style=color:#000>q</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>const</span> <span style=color:#204a87;font-weight:700>double</span> <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>double</span> <span style=color:#ce5c00;font-weight:700>*</span><span style=color:#000>Y</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>size_t</span> <span style=color:#000>n</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>    
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic>// create buffers from malloc allocations to make data accessible from device
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>    <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>buffer</span><span style=color:#ce5c00;font-weight:700>&lt;</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>double</span><span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#000>buf_X</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>n</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>    <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>buffer</span><span style=color:#ce5c00;font-weight:700>&lt;</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#204a87;font-weight:700>double</span><span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#000>buf_Y</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>Y</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>n</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#000>q</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>submit</span><span style=color:#000;font-weight:700>([</span><span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>handler</span> <span style=color:#ce5c00;font-weight:700>&amp;</span><span style=color:#000>cgh</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#8f5902;font-style:italic>// create buffer accessors indicating kernel data-flow pattern  
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>        <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>accessor</span> <span style=color:#000>acc_X</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>buf_X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>cgh</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>read_only</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>        <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>accessor</span> <span style=color:#000>acc_Y</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>buf_Y</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>cgh</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>write_only</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>no_init</span><span style=color:#000;font-weight:700>);</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#000>cgh</span><span style=color:#000;font-weight:700>.</span><span style=color:#000>parallel_for</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>n</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>           <span style=color:#8f5902;font-style:italic>// lambda function that gets executed by different work-items with 
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>           <span style=color:#8f5902;font-style:italic>// different arguments in parallel
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span>           <span style=color:#000;font-weight:700>[</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>sycl</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>id</span><span style=color:#ce5c00;font-weight:700>&lt;</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#ce5c00;font-weight:700>&gt;</span> <span style=color:#000>id</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>              <span style=color:#204a87;font-weight:700>auto</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>accX</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>id</span><span style=color:#000;font-weight:700>];</span>
</span></span><span style=display:flex><span>              <span style=color:#000>accY</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>id</span><span style=color:#000;font-weight:700>]</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>3.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>1.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>-</span><span style=color:#0000cf;font-weight:700>0.5</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#0000cf;font-weight:700>0.3</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>x</span><span style=color:#000;font-weight:700>));</span>
</span></span><span style=display:flex><span>           <span style=color:#000;font-weight:700>});</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}).</span><span style=color:#000>wait</span><span style=color:#000;font-weight:700>();</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>return</span><span style=color:#000;font-weight:700>;</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><p>We refer an interested reader to an excellent and freely available &ldquo;<a href=https://link.springer.com/book/10.1007%2F978-1-4842-5574-2>Data Parallel C++</a>&rdquo; book for
details of this data parallel C++.</p><p>Our package <code>numba_dpex</code> allows one to write kernels directly in Python.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>numba_dpex</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#5c35cc;font-weight:700>@numba_dpex.kernel</span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>numba_dpex_poly</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>    <span style=color:#000>i</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>numba_dpex</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>get_global_id</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>    <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>X</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>i</span><span style=color:#000;font-weight:700>]</span>
</span></span><span style=display:flex><span>    <span style=color:#000>Y</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>i</span><span style=color:#000;font-weight:700>]</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#0000cf;font-weight:700>3.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>1.0</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#000>x</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>-</span><span style=color:#0000cf;font-weight:700>0.5</span> <span style=color:#ce5c00;font-weight:700>+</span> <span style=color:#0000cf;font-weight:700>0.3</span> <span style=color:#ce5c00;font-weight:700>*</span> <span style=color:#000>x</span><span style=color:#000;font-weight:700>))</span>
</span></span></code></pre></div><p>Specifying the execution queue is done using Python context manager:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>numpy</span> <span style=color:#204a87;font-weight:700>as</span> <span style=color:#000>np</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>X</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>random</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>randn</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>10</span><span style=color:#ce5c00;font-weight:700>**</span><span style=color:#0000cf;font-weight:700>6</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#000>Y</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>empty_like</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>with</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>device_context</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>q</span><span style=color:#000;font-weight:700>):</span>
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic># apply the kernel to elements of X, writing value into Y, </span>
</span></span><span style=display:flex><span>    <span style=color:#8f5902;font-style:italic># while executing using given queue</span>
</span></span><span style=display:flex><span>    <span style=color:#000>numba_dpex_poly</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>X</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>numba_dpex</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>DEFAULT_LOCAL_SIZE</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>The argument to <code>device_context</code> can be a queue object, a device object for which a temporary queue will be created,
or a filter selector string. Thus we could have equally used <code>dpctl.device_context(gpu_dev)</code> or <code>dpctl.device_context("gpu")</code>.</p><p>Note that in this examples data sharing was implicitly managed for us: in the case of calling a function from a precompiled
oneAPI native extension data sharing was managed by DPC++ runtime, while in the case of using <code>numba_dpex</code> kernel it was managed
during execution of <code>__call__</code> method.</p><h3 id=data-sharing>Data sharing</h3><p>Implicit management of data is surely convenient, but its use in an interpreted code comes at a performance cost. A runtime must
implicitly copy data from host to the device before the kernel execution commences and then copy some (or all) of it back
after the execution completes for every Python API call.</p><p><code>dpctl</code> provides for allocating memory directly accessible to kernels executing on a device using SYCL&rsquo;s
Unified Shared Memory (<a href=https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:usm>USM</a>) feature. It also implements USM-based ND-array
object <code>dpctl.tensor.usm_ndarray</code> that conforms <a href=https://data-apis.org/array-api/latest/>array-API standard</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>dpctl.tensor</span> <span style=color:#204a87;font-weight:700>as</span> <span style=color:#000>dpt</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># allocate array of doubles using USM-device allocation on GPU device</span>
</span></span><span style=display:flex><span><span style=color:#000>X</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpt</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>arange</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>0.</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>end</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1.0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>step</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1e-6</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>device</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;gpu&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>usm_type</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;device&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># allocate array for the output</span>
</span></span><span style=display:flex><span><span style=color:#000>Y</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>dpt</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>empty_like</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># execution queue is inferred from allocation queues.</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Kernel is executed on the same device where arrays were allocated</span>
</span></span><span style=display:flex><span><span style=color:#000>numba_dpex_poly</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>X</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>size</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>numba_dpex</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>DEFAULT_LOCAL_SIZE</span><span style=color:#000;font-weight:700>](</span><span style=color:#000>X</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>Y</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><p>The execution queue can be unambiguously determined in this case since both arguments are
USM arrays with the same allocation queues and <code>X.sycl_queue == Y.sycl_queue</code> evaluates to <code>True</code>.
Should allocation queues be different, such an inference becomes ambiguous and <code>numba_dpex</code> raises
<code>IndeterminateExecutionQueueError</code> advising user to explicitly migrate the data.</p><p>Migration can be accomplished either by using <code>dpctl.tensor.asarray(X, device=target_device)</code>
to create a copy, or by using <code>X.to_device(target_device)</code> method.</p><p>A USM array can be copied back into a NumPy array using <code>dpt.asnumpy(Y)</code> if needed.</p><h3 id=compute-follows-data>Compute follows data</h3><p>Automatic deduction of the execution queue from allocation queues is consitent with &ldquo;<a href=https://data-apis.org/array-api/latest/design_topics/device_support.html>local control for data allocation target</a>&rdquo;
in the array API standard. User has full control over memory allocation through three keyword arguments present in all <a href=https://data-apis.org/array-api/latest/API_specification/creation_functions.html>array creation functions</a>.
For example, consider</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use usm_type = &#39;device&#39; to get USM-device allocation (default), </span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#     usm_type = &#39;shared&#39; to get USM-shared allocation,</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#     usm_type = &#39;host&#39;   to get USM-host allocation</span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>tensor</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>empty</span><span style=color:#000;font-weight:700>(</span><span style=color:#ce5c00;font-weight:700>...</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>device</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>usm_type</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>sycl_queue</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>None</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>-&gt;</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>tensor</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>usm_ndarray</span><span style=color:#000;font-weight:700>:</span> <span style=color:#ce5c00;font-weight:700>...</span>
</span></span></code></pre></div><p>The keyword <code>device</code> is <a href=https://data-apis.org/array-api/latest/design_topics/device_support.html#syntax-for-device-assignment>mandated by the array API</a>. In <code>dpctl.tensor</code> the allowed values of the keyword are</p><ul><li>Filter selector string, e.g. <code>device="gpu:0"</code></li><li>Existing <code>dpctl.SyclDevice</code> object, e.g. <code>device=dev_gpu</code></li><li>Existing <code>dpctl.SyclQueue</code> object</li><li><code>dpctl.tensor.Device</code> object instance obtained from an existing USM array, e.g. <code>device=X.device</code></li></ul><p>In all cases, an allocation queue object will be constructed as described <a href=#specifying-offload-target>earlier</a> and
stored in the array instance, accessible with <code>X.sycl_queue</code>. Instead of using <code>device</code> keyword, one can alternatively
use <code>sycl_queue</code> keyword for readability to directly specify a <code>dpctl.SyclQueue</code> object to be used as the allocation queue.</p><p>The rationale for storing the allocation queue in the array is that kernels submitted to this queue are guaranteed to be able to
correctly dereference (i.e. access) the USM pointer. Array operations that only involve this single USM array can thus execute
on the allocation queue, and the output array can be allocated on this same allocation queue with the same usm type as the input array.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Reusing the allocation queue of the input array ensures the computational tasks behind the API call can access the array without making
implicit copies and the output array is allocated on the same device as the input.</div><p>Compute follows data is the rule prescribing deduction of the execution and the allocation queue as well as the USM type for the result
when multiple USM arrays are combined. It stipulates that arrays can be combined if and only if their allocation <em>queues are the same</em> as measured
by <code>==</code> operator (i.e. <code>X.sycl_queue == Y.sycl_queue</code> must evaluate to <code>True</code>). Same queues refer to the same underlying task graphs and DPC++ schedulers.</p><p>An attempt to combine USM arrays with unsame allocation queues raises an exception advising the user to migrate the data.
Migration can be accomplished either by using <code>dpctl.tensor.asarray(X, device=Y.device)</code> to create a copy, or by
using <code>X.to_device(Y.device)</code> method which can sometime do the migration more efficiently.</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Advisory</h4><code>dpctl</code> and <code>numba_dpex</code> are both under heavy development. Feel free to file an issue on GitHub or
reach out on Gitter should you encounter any issues.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-430d39d3eb6a31517cc3fc250eafde0d>2 - What is oneAPI</h1><div class=lead>oneAPI - the standard and its implementation.</div><p><a href=https://www.oneapi.io>oneAPI</a> is an open standard for a unified application
programming interface (API) that delivers a common developer experience across
accelerator architectures, including multi-core CPUs, GPUs, and FPGAs.</p><h3 id=toolkits>Toolkits</h3><p>A freely available implementation of the standard is available through
<a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/toolkits.html>Intel(R) oneAPI Toolkits</a>. The <a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html>Intel(R) Base Toolkit</a> features
an industry-leading C++ compiler that implements <a href=https://www.khronos.org/sycl/>SYCL*</a>, an evolution of C++
for heterogeneous computing. It also includes a suite of performance libraries, such as
Intel(R) oneAPI Math Kernel Library (<a href=https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/api-based-programming/intel-oneapi-math-kernel-library-onemkl.html>oneMKL</a>), etc, as well as
<a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-for-python.html>Intel(R) Distribution for Python*</a>.</p><p><img src=../../images/oneapi_basekit.webp alt="Intel Base Toolkit" title="Composition of Intel Base Toolkit"></p><p>DPC++ is a LLVM-based compiler project that implements compiler and runtime support for SYCL* language.
It is being developed in <code>sycl</code> branch of the LLVM project fork <a href=https://github.com/intel/llvm.git>github.com/intel/llvm</a>.
The project publishes <a href=https://github.com/intel/llvm/releases>daily builds</a> for Linux.</p><p>Intel(R) oneAPI DPC++ compiler is a proprietary product that builds on the open-source DPC++ project.
It is part of Intel(R) compiler suite which has completed the <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/adoption-of-llvm-complete-icx.html>adoption of LLVM infrastructure</a> and is available in oneAPI toolkits.
In particular, Intel(R) Fortran compiler is freely avialable on all supported platforms in <a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit.html>Intel(R) oneAPI HPC Toolkit</a>.</p><p>DPC++ leverages standard toolchain runtime libraries, such as <code>glibc</code> and <code>libstdc++</code> on Linux and <code>wincrt</code> on Windows. This makes it possible to use
Intel C/C++ compilers, including DPC++, to compile Python <a href=skbuild.mc>native extensions</a> compatible with the CPython and the rest of Python stack.</p><p>In order to enable cross-architecture programming for CPUs and accelerators the DPC++ runtime adopted <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/expanding-oneapi-support-for-languages-and-accelerators.html>layered architecture</a>.
Software concepts are mapped to hardware abstraction layer by user-specified <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/five-outstanding-additions-sycl2020.html>SYCL backend</a> which programs the specific hardware in use.</p><h3 id=compute-runtime>Compute runtime</h3><p>An integral part of this layered architecture is provided by <a href=https://github.com/intel/compute-runtime.git>Intel(R) Compute Runtime</a>. oneAPI application is a fat binary consisting of
device codes in a standardized intermediate form <a href=https://www.khronos.org/spir/>SPIR-V</a> and host code which orchestrates tasks such as querying of the heterogeneous system it is
running on, selecting accelerator(s), compiling (jitting) device code in the intermediate representation for the selected device, managing device memory, and
submitting compiled device code for execution. The host code performs these tasks by using DPC++ runtime, which maps them to hardware abstraction layer, that
talks to hardware-specific drivers.</p><p><img src=../../images/oneAPI-executable-diagram.webp alt="working of oneAPI executable"></p><h3 id=additional-information>Additional information</h3><p><a href=https://link.springer.com/book/10.1007%2F978-1-4842-5574-2>Data Parallel C++ book</a> is an excellent resource to get familiar with programming
heterogeneous systems using C++ and SYCL*.</p><p>Intel(R) DevCloud hosts <a href=https://devcloud.intel.com/oneapi/get_started/baseTrainingModules/>base training material</a> which can be executed
on the variety of Intel(R) hardware using preinstalled oneAPI toolkits.</p><p>Julia has support for oneAPI <a href=https://github.com/JuliaGPU/oneAPI.jl>github.com/JuliaGPU/oneAPI.jl</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5623910d18a8223f7729c493aeec874c>3 - Features Summary</h1><div class=lead>A list of the main features of the data-parallel extensions to Python packages.</div><h2 id=cross-platform-native-extensions>Cross-platform Native Extensions</h2><p>DPC++ lets you build cross-platform libraries that can be run on a growing set
of heterogeneous devices supported by the compiler, such as Intel CPUs, GPUs,
and FPGAs, and also Nvidia GPUs and AMD GPUs.</p><p>Our package <a href=https://intelpython.github.io/dpctl/latest/index.html>dpctl</a> provides the necessary Python bindings to make a SYCL
library into a Python native extension and subsequently use it from Python.</p><h2 id=write-kernels-directly-in-python>Write Kernels Directly in Python</h2><p>If C++ is not your language, you can skip writing data-parallel kernels in SYCL
and directly write them in Python.</p><p>Our package <a href=https://intelpython.github.io/numba-dpex/latest/index.html>numba-dpex</a> extends the Numba compiler to allow kernel
creation directly in Python via a custom compute API.</p><h2 id=cross-architecture-array-api>Cross-architecture Array API</h2><p>Python array library targeting conformance to core <a href=https://data-apis.org/array-api/latest/>Python Array API</a>
specification.</p><p><a href=https://intelpython.github.io/dpctl/latest/docfiles/dpctl/dpctl.tensor_pyapi.html#dpctl-tensor-pyapi>dpctl.tensor</a> is a Python native extension library implemented using
SYCL within <a href=https://intelpython.github.io/dpctl/latest/index.html>dptcl</a>. The library lets Python users get their job done
using tensor operations powered by pure SYCL generic kernels for portability.</p><h2 id=easy-to-install>Easy to Install</h2><p>All the data-parallel extensions for Python packages are readily available for
installation on conda, PyPI, or github.</p><p>oneAPI Intel LLVM compilers, including DPC++, as well as associated runtimes are
available on conda to support present and future data-parallel extensions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4498e1017b824e5369db44363539378c>4 - Install Intel(R) oneAPI toolkits</h1><div class=lead>Pointers about how to get Intel(R) oneAPI toolkits.</div><h1 id=installation-of-intelr-oneapi-toolkits>Installation of Intel(R) oneAPI toolkits</h1><h2 id=use-intelr-devcloud>Use Intel(R) DevCloud</h2><p><a href=https://devcloud.intel.com/oneapi/>Get free access</a> to a development sandbox with preinstalled and configured
oneAPI toolkits as well as access variety of Intel hardware. This is great and low effort way
to start exploring oneAPI. We recommend to start with <a href="https://jupyter.oneapi.devcloud.intel.com/hub/login?next=/lab/tree/Welcome.ipynb?reset">Jupyter Lab</a>.</p><h2 id=install-locally>Install locally</h2><p>To add oneAPI to your local toolbox, download and install the basekit for your operating system from <a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html>download page</a>.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>For Linux*, toolkits can be installed using OS&rsquo;s package managers, as well as tried out from within a docker-container. Please refer
to the download page for specifics.</div><p>Make sure to configure your system by following steps from &ldquo;<a href=https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html#gs.3mwueb>Get Started Guide</a>&rdquo;
document applicable for your operating system.</p><h2 id=install-in-ci>Install in CI</h2><p>oneAPI can be installed into Linux-powered CI by using the OS&rsquo;s package manager and installing
only the necessary components from required toolkits.</p><p>See <a href=https://github.com/IntelPython/dpctl/blob/1f8e4b35c3d623bd7e0d84dad32f421aef34ac0f/.github/workflows/generate-docs.yml#L18-L29>this example</a> of installing DPC++ compiler in GitHub actions
for <a href=https://github.com/IntelPython/dpctl>IntelPython/dpctl</a> project.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cd3f9e926b7bc350bedceab59f4bbef9>5 - oneAPI Python extensions</h1><div class=lead>Python extensions can be built with DPC++. This is how.</div><h3 id=suitability-of-dpc-for-python-stack>Suitability of DPC++ for Python stack</h3><p>DPC++ is a <a href=https://oneapi-src.github.io/DPCPP_Reference/#data-parallel-c-dpc>single source compiler</a>. It generates <a href=https://www.intel.com/content/www/us/en/develop/documentation/oneapi-programming-guide/top/programming-interface/compilation-flow-overview.html>both the host code and the device code</a> in a single fat binary.</p><p>DPC++ is an LLVM-based compiler, but the host portion of the binary it produces is compatible with GCC runtime libraries on Linux and Windows runtime libraries on Windows. Thus, native Python extensions authored in C++ can be directly built with DPC++. Such extensions will require DPC++ runtime library at the runtime.</p><p>Intel(R) compute runtime needs to be present for DPC++ runtime to be able to target supported Intel devices. When using open-source DPC++ from <a href=https://github.com/intel/llvm.git>github.com/intel/llvm</a> compiled with support for NVIDIA CUDA, HIP NVIDIA, or HIP AMD (see <a href=https://github.com/intel/llvm/blob/sycl/sycl/doc/GetStartedGuide.md>intel/llvm/getting-started</a> for details), respective runtimes and drivers will need to be present for DPC++ runtime to target these devices.</p><h3 id=build-a-data-parallel-python-native-extension>Build a data-parallel Python native extension</h3><p>There are two supported ways of building a data-parallel extension: by using
<code>Cython</code> and by using <code>pybind11</code>. The companion repository
<a href=https://github.com/IntelPython/sample-data-parallel-extensions>IntelPython/sample-data-parallel-extensions</a> provides the
examples demonstrating both approaches by implementing two prototype native
extensions to evaluate <a href=https://en.wikipedia.org/wiki/Kernel_density_estimation>Kernel Density Estimate</a>
at a set a points from a Python function with the following signature:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>def</span> <span style=color:#000>kde_eval</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>exec_q</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>dpctl</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SyclQueue</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>x</span> <span style=color:#000;font-weight:700>:</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>ndarray</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>data</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>ndarray</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>h</span> <span style=color:#000;font-weight:700>:</span> <span style=color:#204a87>float</span><span style=color:#000;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>-&gt;</span> <span style=color:#000>np</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>narray</span><span style=color:#000;font-weight:700>:</span> <span style=color:#ce5c00;font-weight:700>...</span>
</span></span><span style=display:flex><span>    <span style=color:#4e9a06>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    Args:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>       q: execution queue specifying offload target
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>       x: NumPy array of shape (n, dim)
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>       d: NumPy array of shape (n_data, dim)
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>       h: moothing parameter
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>    &#34;&#34;&#34;</span>
</span></span></code></pre></div><p>The examples can be cloned locally using git:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/IntelPython/sample-data-parallel-extensions.git
</span></span></code></pre></div><p>The examples demonstrate a key benefit of using the <code>dpctl</code> package and the
included <code>Cython</code> and <code>pybind11</code> bindings for oneAPI. By using <code>dpctl</code>,
a native extension writer can focus on writing a data-parallel kernel in DPC++
while automating the generation of the necessary Python bindings using <code>dpctl</code>.</p><h4 id=building-packages-with-setuptools>Building packages with setuptools</h4><p>When using <code>setuptools</code> we <a href=https://github.com/IntelPython/sample-data-parallel-extensions/tree/main/kde_setuptools>used</a> environment variables <code>CC</code> and
<code>LDSHARED</code> recognized by <code>setuptools</code> to ensure that <code>dpcpp</code> is used to compile
and link extensions.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#000>CC</span><span style=color:#ce5c00;font-weight:700>=</span>dpcpp <span style=color:#000>LDSHARED</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;dpcpp --shared&#34;</span> python setup.py develop
</span></span></code></pre></div><p>The resulting extension is a fat binary, containing both the host code with
Python bindings and offloading orchestration, and the device code usually stored
in cross-platform intermediate representation (SPIR-V) and compiled for the
device indicated via the execution queue argument using tooling from compute
runtime.</p><h4 id=building-packages-with-scikit-build>Building packages with scikit-build</h4><p>Using setuptools is convenient, but may feel klunky. Using
<a href=https://github.com/scikit-build/scikit-build>scikit-build</a> offers an alternate way for users who prefer or are
familiar with <code>CMake</code>.</p><p>Scikit-build enables writing the logic of Python package building in CMake which <a href=https://www.intel.com/content/www/us/en/develop/documentation/oneapi-dpcpp-cpp-compiler-dev-guide-and-reference/top/compiler-setup/use-the-command-line/use-cmake-with-the-intel-oneapi-dpc-c-compiler.html>supports oneAPI DPC++</a>. Scikit-build supports building of both
Cython-generated and pybind11-generated native extensions. <code>dpctl</code> integration with CMake allows to conveniently using <code>dpctl</code> integration with these extension generators
simply by including</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmake data-lang=cmake><span style=display:flex><span><span style=color:#204a87>find_package</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>Dpctl</span> <span style=color:#4e9a06>REQUIRED</span><span style=color:#000;font-weight:700>)</span><span style=color:#a40000>
</span></span></span></code></pre></div><p>In order for CMake to locate the script that would make the example work, the
example <code>CMakeLists.txt</code> in <a href=https://github.com/IntelPython/sample-data-parallel-extensions/tree/main/kde_skbuild><code>kde_skbuild</code></a> package implements
<code>DPCTL_MODULE_PATH</code> variable which can be set to output of <code>python -m dpctl --cmakedir</code>. Integration of DPC++ with CMake requires that CMake&rsquo;s C and/or C++
compiler were set to Intel LLVM compilers provided in oneAPI base kit.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python setup.py develop -G Ninja -- <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    -DCMAKE_C_COMPILER<span style=color:#ce5c00;font-weight:700>=</span>icx          <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    -DCMAKE_CXX_COMPILER<span style=color:#ce5c00;font-weight:700>=</span>icpx       <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    -DDPCTL_MODULE_PATH<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>$(</span>python -m dpctl --cmakedir<span style=color:#204a87;font-weight:700>)</span>
</span></span></code></pre></div><p>Alteratively, we can rely on CMake recognizing <code>CC</code> and <code>CXX</code> environment variables to shorten the input</p><pre tabindex=0><code>CC=icx CXX=icpx python setup.py develop -G Ninja -- -DDCPTL_MODULE_PATH=$(python -m dpctl --cmakedir)
</code></pre><p>Whichever way of building the data-parallel extension appeals to you, the end
result allows offloading computations specified as DPC++ kernels to any
supported device:</p><pre tabindex=0><code class=language-ipython data-lang=ipython>import dpctl
import numpy as np
import kde_skbuild as kde

cpu_q = dpctl.SyclQueue(&#34;cpu&#34;)
gpu_q = dpctl.SyclQueue(&#34;gpu&#34;)

# output info about targeted devices
cpu_q.print_device_info()
gpu_q.print_device_info()

x = np.linspace(0.1, 0.9, num=14000)
data = np.random.uniform(0, 1, size=10**6)

# Notice that first evaluation results in JIT-compiling the kernel
# Subsequent evaluation reuse cached binary
f0 = kde.cython_kde_eval(cpu_q, x[:, np.newaxis], data[:, np.newaxis], 3e-6)

f1 = kde.cython_kde_eval(gpu_q, x[:, np.newaxis], data[:, np.newaxis], 3e-6)

assert np.allclose(f0, f1)
</code></pre><p>The following naive NumPy implementation can be used to validate the results
generated by our sample extensions. Do note that the validation script would
not be able to handle very large size inputs and will raise a <code>MemoryError</code>
exception.</p><pre tabindex=0><code>def ref_kde(x, data, h):
    &#34;&#34;&#34;
    Reference NumPy implementation for KDE evaluation
    &#34;&#34;&#34;
    assert x.ndim == 2 and data.ndim == 2
    assert x.shape[1] == data.shape[1]
    dim = x.shape[1]
    n_data = data.shape[0]
    return np.exp(
        np.square(x[:, np.newaxis, :]-data).sum(axis=-1)/(-2*h*h)
    ).sum(axis=1)/(np.sqrt(2*np.pi)*h)**dim / n_data
</code></pre><p>Using CPU offload target allows to parallelize CPU computations. For example, try</p><pre tabindex=0><code class=language-ipython data-lang=ipython>data = np.random.uniform(0, 1, size=10**3)
x = np.linspace(0.1, 0.9, num=140)
h = 3e-3

%time fr = ref_kde(x[:,np.newaxis], data[:, np.newaxis], h)
%time f0 = kde_skbuild.cython_kde_eval(cpu_q, x[:, np.newaxis], data[:, np.newaxis], h)
%time f1 = kde_skbuild.cython_kde_eval(gpu_q, x[:, np.newaxis], data[:, np.newaxis], h)

assert np.allclose(f0, fr) and np.allclose(f1, fr)
</code></pre><p><code>dpctl</code> can be used to build data-parallel Python extensions which functions operating of USM-based arrays.
For example, please refer to <a href=https://github.com/IntelPython/dpctl/tree/master/examples/pybind11/onemkl_gemv>examples/pybind11/onemkl_gemv</a> in dpctl sources.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a87383bce703fba3e69097c5c1cb8546>6 - Programming Model</h1><p>The programming model for the <strong>Data Parallel Extensions for Python</strong> (DPX4Py)
suite derives from the oneAPI <a href=https://en.wikipedia.org/wiki/OneAPI_(compute_acceleration)>programming model</a> for device
offload. In oneAPI, a computation kernel can be specified using generic C++
programming and then the kernel can be offloaded to any device that is supported
by an underlying SYCL runtime. The device to which the kernel is offloaded is
specified using an execution queue when <em>launching</em> the kernel.</p><p>The oneAPI unified programming model brings portability across heterogeneous architectures.
Another important aspect of the programming model is its inherent flexibility
that makes it possible to go beyond portability and even strive for performance
portability. An oneAPI library may be implemented using C++ techniques such as
template metaprogramming or dynamic polymorphism to implement specializations
for a generic kernel. If a kernel is implemented polymorphically, the
specialized implementation will be dispatched based on the execution queue
specified during kernel launch. the oneMKL library is an example of a
performance portable oneAPI library. Figure below shows a <strong>gemv</strong> kernel from the
<a href=https://spec.oneapi.io/versions/latest/elements/oneMKL/source/domains/blas/gemv.html#gemv-usm-version>oneMKL library</a> that can be launched on multiple types of architectures
simply by changing the execution queue.</p><p><strong>TODO: Add gemv figure</strong></p><p>In the oneAPI and SYCL programming model, the device where data is allocated is
not tightly coupled with the device where a kernel is executed. The model allows
for implicit data movement across devices. The design offers flexibility to
oneAPI users many of whom are experienced C++ programmers. When extending the
oneAPI programming model to Python via the DPX4Py suite, we felt the need to
make few adjustments to make the model more suited to the Python programming
language. One of the <a href=https://peps.python.org/pep-0020/>key Python tenets</a> is: <em>explicit is better than
implicit</em>. Following the tenet, a <em>Pythonic</em> programming model for device
offload should allow a programmer to explicitly answer the following two key
questions: <strong>Where is data allocated?</strong>, <strong>Where would the computation occur?</strong>
Moreover, if data needs to be moved to a device a programmer should have
explicit control of any such data movement. These requirements are fulfilled by
a programming model called <em>compute follows data</em>.</p><h3 id=compute-follows-data>Compute follows data</h3><p><strong>TODO:</strong></p><ul><li><p>describe compute follows data</p></li><li><p>cite Array API</p></li><li><p>present example</p></li><li><p>End with a rationale. Mention that it does not violate oneAPI programming
model.</p></li></ul><h3 id=extra-knobs>Extra knobs</h3><p><strong>TODO:</strong></p><ul><li>DPX4Py does support the overall oneAPI programming model. Present current way
of launching kernels in dpex.</li><li>Compute follows data is the prescribed model, but libraries can support
implicit data movement (similar to CuPy or TensorFlow) if the want.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0eac71da689dcff3ca0b49e2618ca8f8>7 - Why use oneAPI in Python</h1><p>Here is a summary of why we think Scientific Python community should embrace oneAPI</p><ol><li>oneAPI is an open, cross-industry, standards-based, unified, multiarchitecture, multi-vendor <a href=https://en.wikipedia.org/wiki/OneAPI_(compute_acceleration)>programming model</a>.</li><li>DPC++ compiler is being developed in open-source, see <a href=http://github.com/intel/llvm>http://github.com/intel/llvm</a>, and is being upstreamed into LLVM project itself.</li><li>Open source compiler supports <a href=https://www.intel.com/content/www/us/en/developer/articles/technical/five-outstanding-additions-sycl2020.html>variety of backends</a>, including oneAPI <a href=https://spec.oneapi.io/level-zero/latest/index.html>Level-Zero</a>, <a href=https://www.khronos.org/opencl/>OpenCL(TM)</a>, NVIDIA(R) <a href=https://developer.nvidia.com/cuda-toolkit>CUDA(R)</a>, and <a href=https://github.com/ROCm-Developer-Tools/HIP>HIP</a>.</li><li><a href=https://github.com/oneapi-src/oneMKL>oneAPI Math Kernel Library (oneMKL) Interfaces</a> supports a collection of third-party libraries associated with
supported backends permitting portability.</li></ol><p>With these features in mind, and DPC++ runtime being compatible with compiler toolchain used to build CPython itself, use of oneAPI
promises to enable Python extensions to leverage a variety of accelerators, while maintaining portability
of Python extensions across different heterogenous systems, from HPC clusters and servers to laptops.</p></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Gitter aria-label=Gitter><a class=text-white target=_blank rel=noopener href=https://gitter.im/Data-Parallel-Python/community aria-label=Gitter><i class="fab fa-gitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow for Intel oneAPI" aria-label="Stack Overflow for Intel oneAPI"><a class=text-white target=_blank rel=noopener href=https://stackoverflow.com/questions/tagged/intel-oneapi aria-label="Stack Overflow for Intel oneAPI"><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/IntelPython/ aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Intel Developer Zone" aria-label="Intel Developer Zone"><a class=text-white target=_blank rel=noopener href=https://community.intel.com/t5/Intel-Distribution-for-Python/bd-p/distribution-python aria-label="Intel Developer Zone"><i class="fas fa-hands-helping"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2023 Intel Corp. All Rights Reserved</small></div></div></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/oneAPI-for-SciPy/js/main.min.57829c4a449c0d377b7a44a808f57cbadb16a81488df2545e1739bf4e356c76d.js integrity="sha256-V4KcSkScDTd7ekSoCPV8utsWqBSI3yVF4XOb9ONWx20=" crossorigin=anonymous></script></body></html>